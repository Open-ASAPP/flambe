!Experiment

name: full-example

pipeline: # Define the list of components to execute
  1_train: !Trainer # This is the top level component to execute the run method on
    dataset: !SSTDataset # dataset is an argument to Trainer.__init__
      transform: # Similarly, transform is an argument to the SSTDataset.__init__
        text: !TextField # Fields produce columns, this particular Textfield produces a single one
          embeddings: glove-twitter-200 # note the native support for gensim embedddings
          embeddings_format: gensim
        label: !LabelField
    train_sampler: !BaseSampler
      batch_size: 32
    val_sampler: !BaseSampler
      batch_size: 512
    model: !TextClassifier
      embedder: !Embedder
        embedding: !Embeddings.from_pretrained
          embeddings: !@ 1_train[dataset].text.embedding_matrix # Link over attributes of previously defined objects
          freeze: True
        embedding_dropout: 0.3
        encoder: !PooledRNNEncoder
          input_size: !@ 1_train[model][embedder][embedding].embedding_dim
          rnn_type: lstm
          n_layers: !g [2, 3, 4]  # Grid search over the number of layers
          hidden_size: 256
          pooling: last
          dropout: 0.3
      output_layer: !SoftmaxLayer
        input_size: !@ 1_train[model][embedder][encoder].rnn.hidden_size
        output_size: !@ 1_train[dataset].label.vocab_size
    loss_fn: !torch.NLLLoss
    metric_fn: !Accuracy
    optimizer: !torch.Adam
      params: !@ 1_train[model].trainable_params
    max_steps: 1
    iter_per_step: 1
  2_evaluate: !Evaluator # We evaluate in a second stage, note the reduce argument at the bottom of the file
    dataset: !@ 1_train[dataset]
    model: !@ 1_train[model]
    metric_fn: !Accuracy
    eval_sampler: !BaseSampler
      batch_size: 512
  3_export: !Exporter  # This component doesn't do anything, but makes it easy to identify the objects needed for later use
    model: !@ 2_evaluate[model] # Note that it is completely optional, as these objects can be fecthed from the saved evaluator step
    text: !@ 2_evaluate[dataset].text

schedulers: # Define how to schedule variants
  1_train: !ray.HyperBandScheduler
reduce: # Only use the best variant in subsequent blocks, meaning only the best Trainer will be linked to the Evaluator stage
  1_train: 1
