!Experiment

name: sst-text-classification

pipeline:

  train: !Trainer
    dataset: !SSTDataset # this is a simple Python object, and the arguments to build it
      transform: # these arguments are passed to the init method
        text: !TextField
        label: !LabelField
    model: !TextClassifier
      embedder: !Embedder
        embedding: !Embeddings  # automatically use pytorch classes
          num_embeddings: !@ train[dataset].text.vocab_size # link to other components, and attributes
          embedding_dim: 300
        embedding_dropout: 0.3
        encoder: !PooledRNNEncoder
          input_size: 300
          n_layers: !~c [2, 3]
          hidden_size: 128
          rnn_type: sru
          dropout: !~u [0., 1.]
      output_layer: !SoftmaxLayer
          input_size: !@ train[model][embedder][encoder].rnn.hidden_size # also use inner-links
          output_size: !@ train[dataset].label.vocab_size
    train_sampler: !BaseSampler
    val_sampler: !BaseSampler
    loss_fn: !torch.NLLLoss
    metric_fn: !Accuracy
    optimizer: !Adam
      params: !@ train[model].trainable_params
    iter_per_step: 2
  2_evaluate: !Evaluator # We evaluate in a second stage, note the reduce argument at the bottom of the file
    dataset: !@ train[dataset]
    model: !@ train[model]
    metric_fn: !Accuracy
    eval_sampler: !BaseSampler
      downsample: 0.01
      batch_size: 512

# Define how to schedule variants
algorithm:
    train: !Hyperband
      max_steps: 2
      step_budget: 6

reduce:
  train: 2
#   train: !
