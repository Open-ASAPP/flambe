!Experiment

name: bert

pipeline:
  finetune: !Trainer
    dataset: !TabularDataset.from_path
      train_path: {top_level}/tests/data/dummy_tabular/train.csv
      val_path: {top_level}/tests/data/dummy_tabular/val.csv
      sep: ','
      transform:
        text: !PretrainedTransformerField
          alias: 'bert-base-uncased'
        label: !LabelField
    train_sampler: !BaseSampler
      batch_size: 16
    val_sampler: !BaseSampler
      batch_size: 16
    model: !TextClassifier
      embedder: !PretrainedTransformerEmbedder
        alias: 'bert-base-uncased'
        pool: True
      output_layer: !SoftmaxLayer
        input_size: !@ teacher[embedder].hidden_size
        output_size: !@ dataset.label.vocab_size
    loss_fn: !torch.NLLLoss
    metric_fn: !Accuracy
    optimizer: !torch.Adam
      params: !@ finetune[model].trainable_params
      lr: 0.00005
    max_steps: 1
    iter_per_step: 1
