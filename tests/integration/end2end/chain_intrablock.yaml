!Pipeline

name: basic-example

stages:
  encoder: !PooledRNNEncoder
    input_size: 300
    rnn_type: lstm
    n_layers: 2
    hidden_size: 256
  dataset: !TabularDataset.from_path
    train_path: {top_level}/tests/data/dummy_tabular/train.csv
    val_path: {top_level}/tests/data/dummy_tabular/val.csv
    test_path: {top_level}/tests/data/dummy_tabular/test.csv
    sep: ','
    transform:
      text: !TextField
      label: !LabelField
  model: !TextClassifier
    embedder: !Embedder
      embedding: !Embeddings
        num_embeddings: !copy dataset.text.vocab_size
        embedding_dim: 300
      encoder: !copy encoder
    output_layer: !SoftmaxLayer
      input_size: !copy model[embedder].encoder.rnn.hidden_size
      output_size: !copy dataset.label.vocab_size
  b1: !Trainer
    dataset: !copy dataset
    train_sampler: !BaseSampler
    val_sampler: !BaseSampler
    model: !copy model
    loss_fn: !torch.nn.NLLLoss
    metric_fn: !Accuracy
    optimizer: !torch.optim.Adam
      params: !ref b1[model].trainable_params
    max_steps: 2
    iter_per_step: 2
  b2: !Trainer
    dataset: !copy b1.dataset
    train_sampler: !BaseSampler
    val_sampler: !BaseSampler
    model: !TextClassifier
      embedder: !Embedder
        embedding: !Embeddings
          num_embeddings: !copy b1.model.embedder.embedding.num_embeddings
          embedding_dim: 300
        encoder: !PooledRNNEncoder
          input_size: !ref b2[model][embedder][embedding].embedding_dim
          # input_size: !choice
            # - !ref b2[model][embedder][embedding].embedding_dim
            # - !copy b1.model.embedder.embedding.embedding_dim
          rnn_type: lstm
          n_layers: !choice [2, 3]
          hidden_size: !ref b2[model][embedder][encoder][input_size]
      output_layer: !SoftmaxLayer
        input_size: !ref b2[model][embedder][encoder].rnn.hidden_size
        output_size: !copy dataset.label.vocab_size
    loss_fn: !torch.nn.NLLLoss
    metric_fn: !Accuracy
    optimizer: !torch.optim.Adam
      params: !ref b2[model].trainable_params
    max_steps: 2
    iter_per_step: 2
  b3: !Evaluator
    dataset: !copy b2.dataset
    model: !copy b2.model
    metric_fn: !Accuracy
    eval_sampler: !BaseSampler
      batch_size: 512

algorithm: # Define how to schedule variants based on a metric
  b2: !Hyperband
    max_steps: 2
    step_budget: 4
  

reduce: # Only use the best variant in subsequent blocks
  b2: 1
