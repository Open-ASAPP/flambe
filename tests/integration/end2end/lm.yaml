!Experiment

name: language_modeling
save_path: .

pipeline:
  1_train: !Trainer
    dataset: !TabularDataset.from_path
      train_path: {top_level}/tests/data/dummy_tabular/train.csv
      val_path: {top_level}/tests/data/dummy_tabular/val.csv
      sep: ','
      columns: [text]
      transform:
        text: !LMField
          eos_token: <eos>
    train_sampler: !BaseSampler
      batch_size: 64
    val_sampler: !BaseSampler
      batch_size: 64
    model: !LanguageModel
      embedder: !Embedder
        embedding: !torch.Embedding
          num_embeddings: !@ 1_train[dataset].text.vocab_size
          embedding_dim: 300
        encoder: !RNNEncoder
          input_size: 300
          rnn_type: lstm
          n_layers: 2
          hidden_size: 256
      output_layer: !SoftmaxLayer
        input_size: !@ 1_train[model][embedder].encoder.rnn.hidden_size
        output_size: !@ 1_train[dataset].text.vocab_size
    loss_fn: !torch.NLLLoss
    metric_fn: !Perplexity
    optimizer: !torch.Adam
      params: !@ 1_train[model].trainable_params
    lower_is_better: True
    max_steps: 1
    iter_per_step: 1